mise à jour complète du projet (à copier dans Replit)
🎯 OBJECTIF

Tu travailles sur le projet `301 Redirect Generator`.

Le cadre du projet évolue. Tu dois :

1. Mettre à jour le fichier `CDC.md` (cahier des charges) pour intégrer une nouvelle fonctionnalité de scraping par défaut
2. Créer le fichier `SCRAPING_STRATEGY.md` contenant la vision produit sur le crawling intelligent
3. Créer le fichier `UX_ENHANCEMENTS.md` avec les nouveaux modes d’entrée utilisateurs proposés
4. Créer une issue GitHub `US006 - Scraper automatiquement les URLs des deux sites` avec tous les critères d’acceptation et contraintes

---

📝 1. MISE À JOUR DE `CDC.md`

Dans la section **"Fonctionnalités V1"**, ajoute :

### 🔄 Scraping automatique (par défaut)
L'utilisateur peut simplement entrer deux URLs racines (ancien site et nouveau site) :
- L’outil explore automatiquement les liens internes (jusqu’à 200 pages)
- Le scraping se fait en profondeur, avec respect des domaines
- Ancienne URL = complète, Nouvelle URL = relative
- Le scraping est le mode d’entrée par défaut, mais peut être désactivé

Et dans **"Entrées supportées"**, ajoute :
- `URL d’un site à crawler automatiquement` (par défaut)
- `Upload de fichier sitemap.xml`
- `Copier-coller de liste d’URLs`
- `Copier-coller CSV ancienne/nouvelle URL`

---

📄 2. CRÉE UN NOUVEAU FICHIER `SCRAPING_STRATEGY.md` avec ce contenu :

# SCRAPING_STRATEGY.md

## 🎯 Objectif

Proposer une option par défaut permettant de **scraper automatiquement** les URLs internes de deux sites (ancien et nouveau), sans intervention technique.

## ✅ Ce que fait le scraping

- Explore tous les liens internes `<a href="...">`
- Nettoie et normalise les URLs
- Ne suit pas les liens externes ou assets
- Ignore les doublons
- Limite le crawl à un maximum de 200 pages par site
- Utilise un `User-Agent` dédié : `301-Redirect-Bot`

## 🔐 Accès préprod / privé

- Le scraping doit pouvoir fonctionner même sur des préproductions privées
- Solutions acceptées :
  - Header `User-Agent: 301-Redirect-Bot`
  - Authentification Basic HTTP
  - Token ou clé d'accès
  - Tunnel (ngrok, Cloudflare Tunnel)

## 🧠 Résumé

- Le scraping est le mode par défaut
- L’utilisateur peut le désactiver si besoin
- Le système doit automatiquement détecter et récupérer les URLs internes
- Si le site est inaccessible, fallback vers input manuel

---

📝 3. CRÉE `UX_ENHANCEMENTS.md` avec ce contenu :

# UX_ENHANCEMENTS.md

## 🧠 Nouveau flow UX proposé

L’utilisateur arrive sur l’outil et voit ceci :

### 🔄 Mode automatique (par défaut)



[ Entrez l’URL de l’ancien site ] → auto-crawl
[ Entrez l’URL du nouveau site ] → auto-crawl
[⚡ Générer mes redirections]


### 👁️ Preview automatique

- Liste des URLs détectées
- Compteur de lignes
- Warning si mismatch

### ⚙️ Options avancées

- ◻️ Coller manuellement deux listes
- ◻️ Uploader deux fichiers sitemap.xml
- ◻️ Coller un CSV brut (ancienne / nouvelle)

## 🧠 Résultat attendu

- UX “no-brainer”
- Valeur immédiate (copier les domaines suffit)
- Fallback intelligent si scraping impossible

---

✅ 4. CRÉE UNE NOUVELLE ISSUE `US006 - Scraper automatiquement les URLs des deux sites` :

**Titre** : US006 - Scraper automatiquement les URLs des deux sites

**En tant que** utilisateur,  
**je veux** entrer deux URL racines (ancien site et nouveau site),  
**afin que** l’outil explore automatiquement leurs pages internes  
**et génère les redirections sans copier-coller manuel**.

### 🎯 Critères d’acceptation :
- Entrée : 2 URLs racines
- Sortie : deux listes d’URLs internes (jusqu’à 200 max)
- Matching ligne par ligne
- Ancienne URL complète / nouvelle relative
- Respect des domaines (pas de liens externes)
- Si erreur ou blocage → fallback automatique vers input manuel
- UX simple, rapide, efficace

---

💡 Tu peux commencer par créer les fonctions :
- `crawl_site(url_root)` → retourne liste des URLs internes
- `detect_internal_links(html, base_url)`
- `normalize_url(url)`

Commence par créer les fichiers demandés. Puis propose la logique dans `generator.py`.