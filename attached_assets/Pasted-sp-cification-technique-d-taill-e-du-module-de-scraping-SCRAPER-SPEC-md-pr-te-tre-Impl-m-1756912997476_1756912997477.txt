spÃ©cification technique dÃ©taillÃ©e du module de scraping (SCRAPER_SPEC.md), prÃªte Ã  Ãªtre :

ðŸ”§ ImplÃ©mentÃ©e par Replit ou un dev Python

ðŸ“ VersionnÃ©e dans le repo (/docs ou Ã  la racine)

ðŸ“œ FidÃ¨le Ã  ta vision produit : scraping comme mode dâ€™entrÃ©e par dÃ©faut

ðŸ“„ SCRAPER_SPEC.md â€“ SpÃ©cification du module de scraping
# ðŸ“„ SCRAPER_SPEC.md â€“ SpÃ©cification du module de scraping

## ðŸ§­ Objectif

Ce module est conÃ§u pour extraire automatiquement la liste des URLs internes dâ€™un site web (ancien ou nouveau) Ã  partir dâ€™une **URL racine**.  
Il permet d'alimenter lâ€™outil 301 Redirect Generator **sans intervention manuelle**.  
Il remplace le copier-coller dâ€™un sitemap ou dâ€™un export GA / SEO par un crawl intelligent.

---

## ðŸ§  Fonction attendue

- L'utilisateur saisit **lâ€™URL dâ€™un ancien site** et **lâ€™URL dâ€™un site nouveau / prÃ©prod**
- Le module effectue un **crawl automatique** jusquâ€™Ã  un certain nombre de pages (par dÃ©faut : 200 max)
- Il retourne :
  - Une **liste dâ€™URLs complÃ¨tes** pour le site A (ancien)
  - Une **liste dâ€™URLs relatives** pour le site B (nouveau / prÃ©prod)

---

## ðŸ§± Stack recommandÃ©e

| Ã‰lÃ©ment | Choix |
|--------|-------|
| Langage | Python 3.x |
| HTTP client | `requests` |
| Parsing HTML | `BeautifulSoup` |
| Crawl queue | `deque` (`collections`) |
| Normalisation URL | `urllib.parse`, `tldextract` |
| Respect des contraintes SEO | Optionnel / dÃ©sactivable (robots.txt, nofollow) |
| Alternatives futures | Interface extensible vers `Playwright`, `Scrapy` |

---

## ðŸ” AccÃ¨s prÃ©prod

PrÃ©voir une capacitÃ© d'accÃ¨s **aux environnements restreints** :

| Cas | Solution |
|-----|----------|
| `robots.txt` bloquant | Overridable avec `User-Agent: 301-Redirect-Bot` |
| Meta `noindex/nofollow` | IgnorÃ©s par dÃ©faut |
| Authentification | Support `Basic Auth` ou `Bearer Token` (Ã  injecter dans headers) |
| PrÃ©prod privÃ©e (non exposÃ©e) | Fallback : copier-coller / upload manuel |

---

## ðŸ“¥ EntrÃ©e attendue

```python
crawl_site(
    url_root: str,
    max_pages: int = 200,
    user_agent: str = "301-Redirect-Bot",
    auth: Optional[Tuple[str, str]] = None,
    headers: Optional[Dict[str, str]] = None
) -> List[str]

ðŸ“¤ Sortie

Une liste dâ€™URLs propres, normalisÃ©es, sans doublon :

[
    "https://vieux-site.com/contact",
    "https://vieux-site.com/hebergement/chalet",
    ...
]


Ou pour le nouveau site :

[
    "/contact",
    "/hebergements/chalets",
    ...
]

ðŸ§ª RÃ¨gles fonctionnelles

Le crawler :

suit les liens internes (<a href>)

ignore les liens externes (autres domaines)

nettoie les URLs (sans query string, trailing slash)

Ã©vite les doublons

ignore les ancres (#)

Le crawl est large mais limitÃ© : max 200 pages par domaine

Les URLs peuvent Ãªtre triÃ©es ou conservÃ©es dans lâ€™ordre dâ€™apparition (au choix)

ðŸ§© Cas Ã  gÃ©rer
Cas	Comportement
HTTP 403 / 404	IgnorÃ© avec log silencieux
Timeout / exception	IgnorÃ© avec log silencieux
Lien vide ou ancre seule (#)	IgnorÃ©
Boucle infinie	EmpÃªchÃ©e via set() de pages visitÃ©es
ðŸ”§ Exemple dâ€™implÃ©mentation de base
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import requests
from collections import deque

def crawl_site(url_root: str, max_pages=200) -> list:
    visited = set()
    to_visit = deque([url_root])
    collected = []

    while to_visit and len(collected) < max_pages:
        url = to_visit.popleft()
        if url in visited:
            continue
        visited.add(url)
        try:
            res = requests.get(url, timeout=5, headers={'User-Agent': '301-Redirect-Bot'})
            soup = BeautifulSoup(res.text, "html.parser")
            for link in soup.find_all("a", href=True):
                href = urljoin(url, link['href'])
                if href.startswith(url_root) and href not in visited:
                    to_visit.append(href)
            collected.append(url)
        except:
            continue
    return collected

ðŸ”„ Fallback si Ã©chec du crawl

Si le site ne rÃ©pond pas / bloquÃ© / inaccessible :

Retourne None ou liste vide

Affiche un message UX : "Le site est inaccessible, veuillez coller les URLs ou importer un fichier sitemap"

ðŸ§ª Tests attendus (Pytest)

test_crawl_site_respects_max_pages()

test_crawl_removes_query_strings_and_fragments()

test_crawl_ignores_external_domains()

test_crawl_with_basic_auth_header()

ðŸ”„ Fichiers concernÃ©s

src/scraper.py â†’ contient crawl_site() et helpers

tests/test_scraper.py â†’ tests unitaires du module

main.py â†’ appelle crawl_site() si mode scraping sÃ©lectionnÃ©

ðŸ§™ Bonus futur

Mode â€œdeep crawlâ€ avec rÃ¨gles personnalisÃ©es

Scraping asynchrone (via aiohttp)

UI avec visualisation des liens

IntÃ©gration dans Fire Salamander ðŸ‘€