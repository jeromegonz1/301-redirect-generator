"""
Interface Streamlit pour le gÃ©nÃ©rateur de redirections 301 avec scraping automatique
"""

import streamlit as st
import csv
import io
from generator import RedirectGenerator
from scraper import crawl_site_with_fallback, WebScraper

# Configuration de la page
st.set_page_config(
    page_title="301 Redirect Generator",
    page_icon="ğŸ¦",
    layout="wide"
)

def display_url_preview(old_urls, new_urls):
    """Affiche un aperÃ§u des URLs dÃ©tectÃ©es"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(f"ğŸ”— Anciennes URLs dÃ©tectÃ©es ({len(old_urls)})")
        if old_urls:
            # Affiche les 10 premiÃ¨res URLs
            preview_old = old_urls[:10]
            for url in preview_old:
                st.text(url)
            if len(old_urls) > 10:
                st.text(f"... et {len(old_urls) - 10} autres")
        else:
            st.warning("Aucune URL dÃ©tectÃ©e")
    
    with col2:
        st.subheader(f"ğŸ¯ Nouvelles URLs dÃ©tectÃ©es ({len(new_urls)})")
        if new_urls:
            # Affiche les 10 premiÃ¨res URLs
            preview_new = new_urls[:10]
            for url in preview_new:
                st.text(url)
            if len(new_urls) > 10:
                st.text(f"... et {len(new_urls) - 10} autres")
        else:
            st.warning("Aucune URL dÃ©tectÃ©e")

def generate_redirections_from_lists(old_urls, new_urls):
    """GÃ©nÃ¨re les redirections Ã  partir de deux listes d'URLs"""
    try:
        generator = RedirectGenerator()
        
        # Convertit les listes en texte pour le gÃ©nÃ©rateur existant
        old_text = '\n'.join(old_urls)
        new_text = '\n'.join(new_urls)
        
        htaccess_content, csv_data = generator.generate_redirections(old_text, new_text)
        return htaccess_content, csv_data, None
        
    except Exception as e:
        return None, None, str(e)

def main():
    """Interface principale avec scraping automatique"""
    
    # Titre
    st.title("ğŸ¦ 301 Redirect Generator")
    st.markdown("**Mode automatique avec scraping intelligent** - Entrez 2 URLs, obtenez vos redirections !")
    st.markdown("---")

    # Mode par dÃ©faut : Scraping automatique
    st.header("ğŸ”„ Mode automatique (recommandÃ©)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ”— URL ancien site")
        old_site_url = st.text_input(
            label="URL ancien site",
            placeholder="https://ancien-site.com",
            label_visibility="collapsed",
            help="L'outil va automatiquement explorer ce site pour trouver toutes les pages"
        )
    
    with col2:
        st.subheader("ğŸ¯ URL nouveau site") 
        new_site_url = st.text_input(
            label="URL nouveau site",
            placeholder="https://nouveau-site.com",
            label_visibility="collapsed", 
            help="L'outil va automatiquement explorer ce site pour trouver toutes les pages"
        )
    
    # Bouton de scraping
    st.markdown("")
    col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 1])
    with col_btn2:
        scrape_button = st.button("âš¡ Scraper les deux sites", use_container_width=True, type="primary")
    
    # Variables de session pour stocker les rÃ©sultats
    if 'scraped_old_urls' not in st.session_state:
        st.session_state.scraped_old_urls = []
    if 'scraped_new_urls' not in st.session_state:
        st.session_state.scraped_new_urls = []
    if 'scraping_success' not in st.session_state:
        st.session_state.scraping_success = False
    
    # Traitement du scraping
    if scrape_button:
        if not old_site_url.strip() or not new_site_url.strip():
            st.error("âŒ Veuillez saisir les deux URLs avant de lancer le scraping.")
        else:
            with st.spinner("ğŸ” Scraping en cours... Cela peut prendre quelques secondes"):
                
                # Scraping de l'ancien site
                st.info("ğŸ“¡ Scraping de l'ancien site...")
                old_urls, old_success = crawl_site_with_fallback(old_site_url, max_pages=200)
                
                # Scraping du nouveau site  
                st.info("ğŸ“¡ Scraping du nouveau site...")
                scraper = WebScraper()
                new_urls_absolute, new_success = crawl_site_with_fallback(new_site_url, max_pages=200)
                
                # Convertit les nouvelles URLs en relatives
                new_urls = []
                if new_success and new_urls_absolute:
                    new_urls = scraper.crawl_site_relative(new_site_url, max_pages=200)
                    if not new_urls:
                        new_success = False
                
                # Stocke les rÃ©sultats
                st.session_state.scraped_old_urls = old_urls
                st.session_state.scraped_new_urls = new_urls
                st.session_state.scraping_success = old_success and new_success
                
                # Affiche les rÃ©sultats du scraping
                if st.session_state.scraping_success:
                    st.success(f"âœ… Scraping rÃ©ussi ! TrouvÃ© {len(old_urls)} pages sur l'ancien site et {len(new_urls)} sur le nouveau.")
                else:
                    st.warning("âš ï¸ Le scraping automatique a Ã©chouÃ© ou n'a pas trouvÃ© d'URLs. Utilisez le mode manuel ci-dessous.")
                    if not old_success:
                        st.error(f"âŒ Impossible de scraper l'ancien site : {old_site_url}")
                    if not new_success:
                        st.error(f"âŒ Impossible de scraper le nouveau site : {new_site_url}")
    
    # Affichage des rÃ©sultats du scraping si disponibles
    if st.session_state.scraped_old_urls or st.session_state.scraped_new_urls:
        st.markdown("---")
        st.subheader("ğŸ‘ï¸ AperÃ§u des URLs dÃ©tectÃ©es")
        
        display_url_preview(st.session_state.scraped_old_urls, st.session_state.scraped_new_urls)
        
        # VÃ©rification de correspondance
        if len(st.session_state.scraped_old_urls) != len(st.session_state.scraped_new_urls):
            st.warning(f"âš ï¸ Nombre d'URLs diffÃ©rent : {len(st.session_state.scraped_old_urls)} vs {len(st.session_state.scraped_new_urls)}. Les redirections seront gÃ©nÃ©rÃ©es pour les URLs correspondantes.")
        
        # Bouton de gÃ©nÃ©ration des redirections
        col_gen1, col_gen2, col_gen3 = st.columns([1, 1, 1])
        with col_gen2:
            generate_from_scraping = st.button("ğŸš€ GÃ©nÃ©rer les redirections", use_container_width=True, type="primary")
        
        if generate_from_scraping:
            if st.session_state.scraped_old_urls and st.session_state.scraped_new_urls:
                htaccess_content, csv_data, error = generate_redirections_from_lists(
                    st.session_state.scraped_old_urls, 
                    st.session_state.scraped_new_urls
                )
                
                if error:
                    st.error(f"âŒ Erreur lors de la gÃ©nÃ©ration: {error}")
                else:
                    # Affichage des rÃ©sultats
                    st.success(f"âœ… {len(csv_data)-1} redirections gÃ©nÃ©rÃ©es avec succÃ¨s !")
                    
                    # AperÃ§u du .htaccess
                    st.subheader("ğŸ“„ AperÃ§u du fichier .htaccess")
                    st.code(htaccess_content, language="apache")
                    
                    # Boutons de tÃ©lÃ©chargement
                    st.markdown("---")
                    st.subheader("ğŸ“¥ TÃ©lÃ©chargements")
                    
                    col_dl1, col_dl2 = st.columns(2)
                    
                    with col_dl1:
                        st.download_button(
                            label="â¬‡ï¸ TÃ©lÃ©charger .htaccess",
                            data=htaccess_content,
                            file_name="redirections.htaccess",
                            mime="text/plain"
                        )
                    
                    with col_dl2:
                        # GÃ©nÃ©ration du CSV
                        csv_buffer = io.StringIO()
                        csv_writer = csv.writer(csv_buffer)
                        csv_writer.writerows(csv_data)
                        csv_content = csv_buffer.getvalue()
                        
                        st.download_button(
                            label="â¬‡ï¸ TÃ©lÃ©charger CSV",
                            data=csv_content,
                            file_name="redirections_audit.csv",
                            mime="text/csv"
                        )
                    
                    # AperÃ§u du CSV
                    st.subheader("ğŸ“Š AperÃ§u du fichier CSV")
                    st.dataframe(csv_data[1:], column_config={
                        "0": "Ancienne URL complÃ¨te",
                        "1": "Ancien chemin", 
                        "2": "Nouvelle URL complÃ¨te",
                        "3": "Nouveau chemin"
                    })
            else:
                st.error("âŒ Aucune URL trouvÃ©e pour gÃ©nÃ©rer les redirections.")

    # Mode manuel (fallback)
    st.markdown("---")
    with st.expander("ğŸ› ï¸ Mode manuel (si le scraping automatique ne fonctionne pas)"):
        st.markdown("**Utilisez ce mode si :**")
        st.markdown("- Le scraping automatique a Ã©chouÃ©")
        st.markdown("- Vous prÃ©fÃ©rez coller vos listes d'URLs manuellement")
        st.markdown("- Vous avez des fichiers sitemap.xml Ã  uploader")
        
        # Tabs pour les diffÃ©rents modes manuels
        tab1, tab2 = st.tabs(["ğŸ“ Copier-coller", "ğŸ“ Upload fichiers"])
        
        with tab1:
            col1_manual, col2_manual = st.columns(2)
            
            with col1_manual:
                st.subheader("ğŸ”— Anciennes URLs")
                old_urls_manual = st.text_area(
                    label="Anciennes URLs",
                    placeholder="Collez ici vos anciennes URLs...\nExemple:\nhttps://ancien-site.com/page1\nhttps://ancien-site.com/page2",
                    height=300,
                    label_visibility="collapsed",
                    help="Formats supportÃ©s: sitemap XML, liste brute, CSV"
                )
            
            with col2_manual:
                st.subheader("ğŸ¯ Nouvelles URLs") 
                new_urls_manual = st.text_area(
                    label="Nouvelles URLs",
                    placeholder="Collez ici vos nouvelles URLs...\nExemple:\nhttps://nouveau-site.com/nouvelle-page1\nhttps://nouveau-site.com/nouvelle-page2",
                    height=300,
                    label_visibility="collapsed",
                    help="Formats supportÃ©s: sitemap XML, liste brute, CSV"
                )
            
            # Bouton de gÃ©nÃ©ration manuelle
            col_manual1, col_manual2, col_manual3 = st.columns([1, 1, 1])
            with col_manual2:
                generate_manual = st.button("ğŸš€ GÃ©nÃ©rer (mode manuel)", use_container_width=True)
            
            if generate_manual:
                if not old_urls_manual.strip() or not new_urls_manual.strip():
                    st.error("âŒ Veuillez remplir les deux champs avant de gÃ©nÃ©rer les redirections.")
                else:
                    try:
                        generator = RedirectGenerator()
                        htaccess_content, csv_data = generator.generate_redirections(old_urls_manual, new_urls_manual)
                        
                        # Affichage des rÃ©sultats (mÃªme code que plus haut)
                        st.success(f"âœ… {len(csv_data)-1} redirections gÃ©nÃ©rÃ©es avec succÃ¨s !")
                        
                        # AperÃ§u du .htaccess
                        st.subheader("ğŸ“„ AperÃ§u du fichier .htaccess")
                        st.code(htaccess_content, language="apache")
                        
                        # Boutons de tÃ©lÃ©chargement
                        st.markdown("---")
                        st.subheader("ğŸ“¥ TÃ©lÃ©chargements")
                        
                        col_dl1, col_dl2 = st.columns(2)
                        
                        with col_dl1:
                            st.download_button(
                                label="â¬‡ï¸ TÃ©lÃ©charger .htaccess",
                                data=htaccess_content,
                                file_name="redirections.htaccess",
                                mime="text/plain"
                            )
                        
                        with col_dl2:
                            csv_buffer = io.StringIO()
                            csv_writer = csv.writer(csv_buffer)
                            csv_writer.writerows(csv_data)
                            csv_content = csv_buffer.getvalue()
                            
                            st.download_button(
                                label="â¬‡ï¸ TÃ©lÃ©charger CSV",
                                data=csv_content,
                                file_name="redirections_audit.csv",
                                mime="text/csv"
                            )
                        
                        # AperÃ§u du CSV
                        st.subheader("ğŸ“Š AperÃ§u du fichier CSV")
                        st.dataframe(csv_data[1:])
                        
                    except ValueError as e:
                        st.error(f"âŒ Erreur de correspondance: {str(e)}")
                        st.info("ğŸ’¡ VÃ©rifiez que vous avez le mÃªme nombre d'URLs dans les deux listes.")
                    except Exception as e:
                        st.error(f"âŒ Erreur lors de la gÃ©nÃ©ration: {str(e)}")
        
        with tab2:
            st.subheader("ğŸ“ Upload de fichiers")
            col_up1, col_up2 = st.columns(2)
            
            with col_up1:
                old_file = st.file_uploader(
                    "Fichier ancien site",
                    type=['txt', 'xml'],
                    help="Fichier texte ou sitemap.xml"
                )
            
            with col_up2:
                new_file = st.file_uploader(
                    "Fichier nouveau site", 
                    type=['txt', 'xml'],
                    help="Fichier texte ou sitemap.xml"
                )
            
            if old_file and new_file:
                if st.button("ğŸš€ GÃ©nÃ©rer depuis fichiers"):
                    try:
                        old_content = old_file.read().decode('utf-8')
                        new_content = new_file.read().decode('utf-8')
                        
                        generator = RedirectGenerator()
                        htaccess_content, csv_data = generator.generate_redirections(old_content, new_content)
                        
                        st.success(f"âœ… {len(csv_data)-1} redirections gÃ©nÃ©rÃ©es depuis les fichiers !")
                        
                        # Interface de tÃ©lÃ©chargement (mÃªme code que plus haut)
                        col_dl1, col_dl2 = st.columns(2)
                        
                        with col_dl1:
                            st.download_button(
                                label="â¬‡ï¸ TÃ©lÃ©charger .htaccess",
                                data=htaccess_content,
                                file_name="redirections.htaccess",
                                mime="text/plain"
                            )
                        
                        with col_dl2:
                            csv_buffer = io.StringIO()
                            csv_writer = csv.writer(csv_buffer)
                            csv_writer.writerows(csv_data)
                            csv_content = csv_buffer.getvalue()
                            
                            st.download_button(
                                label="â¬‡ï¸ TÃ©lÃ©charger CSV",
                                data=csv_content,
                                file_name="redirections_audit.csv",
                                mime="text/csv"
                            )
                        
                    except Exception as e:
                        st.error(f"âŒ Erreur lors du traitement des fichiers: {str(e)}")

    # Instructions d'utilisation
    with st.expander("ğŸ“‹ Instructions & aide"):
        st.markdown("""
        ### ğŸ”„ Mode automatique (recommandÃ©)
        1. **Entrez l'URL de l'ancien site** (ex: https://ancien-site.com)
        2. **Entrez l'URL du nouveau site** (ex: https://nouveau-site.com)
        3. **Cliquez sur "Scraper les deux sites"** - L'outil va automatiquement explorer jusqu'Ã  200 pages de chaque site
        4. **VÃ©rifiez les URLs dÃ©tectÃ©es** dans l'aperÃ§u
        5. **GÃ©nÃ©rez les redirections** et tÃ©lÃ©chargez vos fichiers
        
        ### ğŸ› ï¸ Mode manuel (fallback)
        Utilisez ce mode si le scraping automatique Ã©choue :
        - **Copier-coller** : Collez vos listes d'URLs ou contenus de sitemap
        - **Upload** : Importez vos fichiers sitemap.xml ou listes d'URLs
        
        ### ğŸ“¤ Formats supportÃ©s
        - Sitemap XML (balises `<loc>`)
        - Liste brute (une URL par ligne)
        - CSV copiÃ© (deux colonnes)
        
        ### âš ï¸ DÃ©pannage
        - Si le scraping Ã©choue : vÃ©rifiez que les sites sont accessibles
        - En cas de blocage robots.txt : utilisez le mode manuel
        - Pour des sites avec authentification : contactez l'Ã©quipe technique
        """)
    
    # Footer
    st.markdown("---")
    st.markdown("*DÃ©veloppÃ© pour SEPTEO Digital Services â€” Fire Salamander Team* ğŸ¦")
    st.markdown("*v2.0 avec scraping automatique intelligent*")

if __name__ == "__main__":
    main()