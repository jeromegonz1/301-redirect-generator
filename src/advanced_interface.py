"""
Interface avanc√©e Sprint 2 avec IA s√©mantique et support multilangue
"""

import streamlit as st
import csv
import io
import os
from typing import List, Dict, Any
from urllib.parse import urlparse
from generator import RedirectGenerator
from scraper import crawl_site_with_fallback, WebScraper, parse_sitemap
from language_detector import LanguageDetector
from ai_mapper import AIMapper, AIMatchingError
from fallback_manager import FallbackManager
from cache_manager import CacheManager
from fallback_302_intelligent import Fallback302Intelligent

# Import pour export XLS Balt (Sprint 3.5)
try:
    import openpyxl
    from openpyxl import Workbook
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False


def url_to_relative_path(url: str) -> str:
    """
    Convertit une URL absolue en chemin relatif pour .htaccess
    
    Args:
        url: URL absolue (ex: http://site.com/path/page)
        
    Returns:
        Chemin relatif (ex: /path/page)
    """
    if not url or not isinstance(url, str):
        return "/"
    
    parsed = urlparse(url.strip())
    path = parsed.path if parsed.path else "/"
    
    # Assure qu'on a un slash au d√©but
    if not path.startswith('/'):
        path = '/' + path
    
    return path


def convert_url_domain(old_url: str, target_domain: str) -> str:
    """
    Convertit une URL d'un domaine vers un autre en conservant le path
    
    Args:
        old_url: URL source (ex: https://www.campinglescascades.com/fr/page)
        target_domain: Domaine cible (ex: https://www.les-cascades.com)
        
    Returns:
        URL avec nouveau domaine (ex: https://www.les-cascades.com/fr/page)
    """
    if not old_url or not isinstance(old_url, str):
        return target_domain
    
    parsed = urlparse(old_url.strip())
    path = parsed.path if parsed.path else "/"
    query = f"?{parsed.query}" if parsed.query else ""
    fragment = f"#{parsed.fragment}" if parsed.fragment else ""
    
    # S'assurer que le domaine cible se termine sans slash
    target_domain = target_domain.rstrip('/')
    
    return f"{target_domain}{path}{query}{fragment}"


def extract_language_from_url_path(url: str) -> str:
    """
    Extrait la langue depuis une URL en analysant le path
    
    Args:
        url: URL √† analyser
        
    Returns:
        Code langue (fr, en, de, nl, etc.) ou 'fr' par d√©faut
    """
    if not url:
        return "fr"
    
    path = url_to_relative_path(url)
    
    # Patterns de langue support√©s : /fr/, /en/, /de/, /nl/, /es/, /it/
    languages = ['fr', 'en', 'de', 'nl', 'es', 'it']
    
    for lang in languages:
        if f"/{lang}/" in path or path.startswith(f"/{lang}"):
            return lang
    
    return "fr"  # Fallback par d√©faut


def remove_duplicate_redirects(redirects: List[Dict]) -> List[Dict]:
    """
    Filtre les redirections pour √©viter les doublons et source == cible
    
    Args:
        redirects: Liste des redirections avec 'source' et 'target'
        
    Returns:
        Liste filtr√©e sans doublons
    """
    seen = set()
    filtered = []
    
    for redirect in redirects:
        source = redirect.get('source', '')
        target = redirect.get('target', '')
        
        # Ignorer si source == target  
        if source == target:
            continue
            
        # Ignorer les doublons
        redirect_key = (source, target)
        if redirect_key in seen:
            continue
            
        seen.add(redirect_key)
        filtered.append(redirect)
    
    return filtered


def generate_balt_xlsx(redirects_data: List[Dict], output_path: str = None, output_dir: str = None) -> str:
    """
    G√©n√®re un fichier Excel (.xlsx) au format Balt pour import automatique Septeo
    
    Format Balt:
    - 2 colonnes exactement: Source, Target
    - URLs absolues uniquement (source ET target)
    - Pas de commentaires, confidence ou type redirection
    - UTF-8, nom: redirections_balt.xlsx
    
    Args:
        redirects_data: Liste des redirections avec 'source' et 'target'
        output_path: Chemin complet du fichier de sortie (optionnel)
        output_dir: R√©pertoire de sortie (si output_path non sp√©cifi√©)
        
    Returns:
        Chemin du fichier Excel g√©n√©r√©
        
    Raises:
        ImportError: Si openpyxl n'est pas disponible
        ValueError: Si les donn√©es sont invalides
    """
    if not OPENPYXL_AVAILABLE:
        raise ImportError("openpyxl requis pour l'export XLS. Installation: pip install openpyxl")
    
    # D√©termination du chemin de sortie
    if not output_path:
        if output_dir:
            output_path = os.path.join(output_dir, 'redirections_balt.xlsx')
        else:
            output_path = 'redirections_balt.xlsx'
    
    # Cr√©ation du workbook Excel
    workbook = Workbook()
    worksheet = workbook.active
    worksheet.title = "Redirections"
    
    # En-t√™tes (ligne 1) - Format Balt exact
    worksheet['A1'] = 'Source'
    worksheet['B1'] = 'Target'
    
    # Donn√©es (√† partir de la ligne 2)
    current_row = 2
    for redirect in redirects_data:
        source = redirect.get('source', '')
        target = redirect.get('target', '')
        
        # Validation: URLs absolues uniquement pour format Balt
        if source.startswith('https://') or source.startswith('http://'):
            if target.startswith('https://') or target.startswith('http://'):
                worksheet[f'A{current_row}'] = source
                worksheet[f'B{current_row}'] = target
                current_row += 1
    
    # Sauvegarde du fichier Excel
    try:
        workbook.save(output_path)
        workbook.close()
        
        # V√©rification que le fichier a √©t√© cr√©√©
        if not os.path.exists(output_path):
            raise FileNotFoundError(f"√âchec cr√©ation fichier: {output_path}")
            
        return output_path
        
    except Exception as e:
        # Nettoyage en cas d'erreur
        if 'workbook' in locals():
            workbook.close()
        if os.path.exists(output_path):
            try:
                os.remove(output_path)
            except:
                pass
        raise ValueError(f"Erreur g√©n√©ration XLS Balt: {str(e)}")


def interface_ai_avancee():
    """Interface avanc√©e avec IA s√©mantique et multilangue"""
    
    st.header("üöÄ G√©n√©rateur IA avec support multilangue")
    st.info("‚ÑπÔ∏è Cette interface utilise GPT-3.5-turbo pour le matching s√©mantique intelligent des URLs")
    
    # Initialisation du gestionnaire de cache
    cache_manager = CacheManager()
    
    # V√©rification de la cl√© API
    if not os.getenv("OPENAI_API_KEY"):
        st.error("‚ùå Cl√© API OpenAI manquante. Veuillez la configurer dans les secrets Replit.")
        return
    
    # Configuration IA
    with st.expander("‚öôÔ∏è Configuration IA"):
        col1, col2 = st.columns(2)
        with col1:
            temperature = st.slider("üå°Ô∏è Temp√©rature IA", 0.0, 1.0, 0.1, 0.1,
                                   help="0.0 = conservateur, 1.0 = cr√©atif")
            chunk_size = st.number_input("üì¶ Taille des lots", 10, 50, 20,
                                       help="URLs trait√©es par lot")
        with col2:
            confidence_threshold = st.slider("üéØ Seuil de confiance", 0.5, 0.9, 0.7, 0.05,
                                            help="Score minimum pour valider un match")
            fallback_lang = st.selectbox("üåê Langue de fallback", 
                                       ["fr", "en", "de", "es", "it", "nl"], 
                                       index=0)
    
    # Configuration Fallback 302 Intelligent (Sprint 3)
    with st.expander("üîÑ Fallback intelligent 302 (Sprint 3)"):
        enable_fallback_302 = st.checkbox(
            "‚úÖ Activer fallback 302 automatique", 
            value=True,
            help="Redirige automatiquement les URLs non match√©es vers la page d'accueil de leur langue (302 temporaire)"
        )
        
        if enable_fallback_302:
            st.info("üéØ Les URLs sans correspondance IA seront redirig√©es en 302 vers /langue/")
            st.write("**Exemple :** `/fr/page-orpheline` ‚Üí `Redirect 302 /fr/page-orpheline /fr/`")
        else:
            st.warning("‚ö†Ô∏è URLs non match√©es ignoreront les fallback (peut g√©n√©rer des 404)")
    
    # Gestion du cache
    with st.expander("üíæ Gestion du cache (√©vite les appels API r√©p√©t√©s)"):
        col_cache1, col_cache2 = st.columns(2)
        
        with col_cache1:
            # Statistiques du cache
            cache_stats = cache_manager.get_cache_stats()
            st.metric("R√©sultats en cache", cache_stats["gpt_cache_files"])
            st.metric("Fichiers .htaccess", cache_stats["htaccess_files"])
            
        with col_cache2:
            st.metric("Taille cache", f"{cache_stats['total_size_mb']} MB")
            if st.button("üßπ Nettoyer cache (>7j)"):
                deleted = cache_manager.clear_old_cache()
                st.success(f"‚úÖ {deleted} fichiers supprim√©s")
        
        # Affichage des caches r√©cents
        cached_results = cache_manager.list_cached_results()
        if cached_results:
            st.write("**R√©sultats r√©cents en cache:**")
            for i, cache_info in enumerate(cached_results[:3]):  # Affiche les 3 plus r√©cents
                st.text(f"‚Ä¢ {cache_info['timestamp'][:16]} - {cache_info['old_urls_count']} ‚Üí {cache_info['new_urls_count']} URLs")
    
    # Collecte des URLs
    st.header("üìä Collecte des URLs")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üîó Ancien site")
        old_input_mode = st.radio("Mode", ["URL √† scraper", "Sitemap XML", "Liste manuelle"], key="old")
        
        if old_input_mode == "URL √† scraper":
            old_url = st.text_input("URL de l'ancien site", placeholder="https://ancien-site.com")
            if st.button("üï∑Ô∏è Scraper ancien site"):
                if old_url:
                    with st.spinner("Scraping en cours..."):
                        old_urls = crawl_site_with_fallback(old_url, max_pages=200)
                        st.session_state.old_urls = old_urls
                        st.success(f"‚úÖ {len(old_urls)} URLs collect√©es")
                else:
                    st.error("Veuillez entrer une URL")
        elif old_input_mode == "Sitemap XML":
            old_sitemap_url = st.text_input("URL du sitemap ancien site", placeholder="https://ancien-site.com/sitemap.xml")
            if st.button("üó∫Ô∏è Parser sitemap ancien site"):
                if old_sitemap_url:
                    with st.spinner("Parsing du sitemap..."):
                        old_urls = parse_sitemap(old_sitemap_url)
                        st.session_state.old_urls = old_urls
                        st.success(f"‚úÖ {len(old_urls)} URLs extraites du sitemap")
                else:
                    st.error("Veuillez entrer une URL de sitemap")
        else:
            old_text = st.text_area("URLs (une par ligne)", height=200, key="old_manual")
            if old_text:
                st.session_state.old_urls = [url.strip() for url in old_text.split('\n') if url.strip()]
    
    with col2:
        st.subheader("üéØ Nouveau site")
        new_input_mode = st.radio("Mode", ["Sitemap XML", "Liste manuelle"], key="new")
        
        if new_input_mode == "Sitemap XML":
            sitemap_url = st.text_input("URL du sitemap", placeholder="https://nouveau-site.com/sitemap.xml")
            if st.button("üó∫Ô∏è Parser sitemap"):
                if sitemap_url:
                    with st.spinner("Parsing du sitemap..."):
                        new_urls = parse_sitemap(sitemap_url)
                        st.session_state.new_urls = new_urls
                        st.success(f"‚úÖ {len(new_urls)} URLs extraites")
                else:
                    st.error("Veuillez entrer une URL de sitemap")
        else:
            new_text = st.text_area("URLs (une par ligne)", height=200, key="new_manual")
            if new_text:
                st.session_state.new_urls = [url.strip() for url in new_text.split('\n') if url.strip()]
    
    # Affichage des URLs collect√©es avec analyse linguistique
    if hasattr(st.session_state, 'old_urls') and hasattr(st.session_state, 'new_urls'):
        old_urls = st.session_state.old_urls
        new_urls = st.session_state.new_urls
        
        st.header("üîç Analyse multilangue")
        
        # D√©tection des langues
        detector = LanguageDetector()
        old_grouped = detector.group_by_language(old_urls)
        new_grouped = detector.group_by_language(new_urls)
        
        # Affichage de l'analyse
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("üìä Total ancien", len(old_urls))
            st.metric("üåê Langues ancien", len(old_grouped))
            
        with col2:
            st.metric("üìä Total nouveau", len(new_urls))
            st.metric("üåê Langues nouveau", len(new_grouped))
            
        with col3:
            # Calcul des langues manquantes
            fallback_manager = FallbackManager(fallback_lang)
            missing_langs = fallback_manager.detect_missing_languages(old_urls, new_urls)
            st.metric("‚ö†Ô∏è Langues manquantes", len(missing_langs))
            if missing_langs:
                st.warning(f"Langues non migr√©es: {', '.join(missing_langs)}")
        
        # D√©tails par langue
        with st.expander("üìà D√©tails par langue"):
            all_langs = set(old_grouped.keys()) | set(new_grouped.keys())
            
            for lang in sorted(all_langs):
                old_count = len(old_grouped.get(lang, []))
                new_count = len(new_grouped.get(lang, []))
                status = "‚úÖ Migr√©e" if new_count > 0 else "‚ùå Manquante"
                
                st.write(f"**{lang.upper()}** - Ancien: {old_count}, Nouveau: {new_count} - {status}")
        
        # Configuration domaine cible
        st.header("üåê Configuration domaine cible")
        target_domain = st.text_input(
            "Domaine cible pour les redirections",
            placeholder="https://www.les-cascades.com",
            help="Domaine complet du nouveau site (sans slash final)",
            value="https://www.les-cascades.com"  # Valeur par d√©faut
        )
        
        if not target_domain:
            st.error("‚ö†Ô∏è Domaine cible requis pour g√©n√©rer les redirections absolues")
            return
        
        # Configuration du contexte m√©tier
        st.header("üíº Contexte m√©tier (optionnel)")
        contexte_metier = st.text_area(
            "Informations pour am√©liorer le matching IA",
            placeholder="""Exemple:
- Refonte compl√®te avec nouvelle arborescence
- Section 'Actualit√©s' devient 'Blog'  
- Produits regroup√©s par cat√©gories
- URLs en fran√ßais uniquement sur nouveau site""",
            height=100
        )
        
        # G√©n√©ration avec IA
        if st.button("ü§ñ G√©n√©rer avec IA s√©mantique", type="primary"):
            with st.spinner("üß† V√©rification du cache..."):
                try:
                    # V√©rification du cache d'abord
                    cache_key_info = f"Temp:{temperature}, Chunk:{chunk_size}, Seuil:{confidence_threshold}"
                    cached_result = cache_manager.get_gpt_cache(
                        old_urls, new_urls, contexte_metier, temperature
                    )
                    
                    if cached_result:
                        st.success(f"‚úÖ R√©sultats trouv√©s en cache ! (√âconomie API)")
                        # Utilise les r√©sultats du cache
                        all_matches = cached_result["results"]["all_matches"]
                        all_unmatched = cached_result["results"]["all_unmatched"]
                        missing_langs = cached_result["results"]["missing_langs"]
                        old_grouped = cached_result["results"]["old_grouped"]
                        
                        st.info(f"üìä Cache utilis√©: {len(all_matches)} correspondances trouv√©es")
                    else:
                        st.info("üí∏ Appel API GPT n√©cessaire - Nouveau matching...")
                        
                        # Initialisation de l'IA
                        ai_mapper = AIMapper(
                            api_key=os.getenv("OPENAI_API_KEY"),
                            temperature=temperature,
                            chunk_size=chunk_size
                        )
                    
                        # G√©n√©ration du rapport de fallback d'abord
                        if missing_langs:
                            st.subheader("üìã Rapport des redirections temporaires")
                            fallback_report = fallback_manager.generate_fallback_report(old_urls, new_urls)
                            st.text(fallback_report)
                        
                        # Matching IA pour chaque langue commune
                        st.subheader("ü§ñ R√©sultats du matching IA")
                        
                        all_matches = []
                        all_unmatched = []
                        
                        common_langs = set(old_grouped.keys()) & set(new_grouped.keys())
                        
                        for lang in sorted(common_langs):
                            if old_grouped[lang] and new_grouped[lang]:
                                st.write(f"**üîÑ Traitement langue: {lang.upper()}**")
                                
                                # Matching IA pour cette langue
                                result = ai_mapper.match_urls(
                                    old_grouped[lang],
                                    new_grouped[lang],
                                    contexte_metier=contexte_metier,
                                    langue=lang
                                )
                                
                                # Affichage des r√©sultats
                                matches = result.correspondances
                                unmatched = result.non_matchees
                                
                                st.success(f"‚úÖ {len(matches)} correspondances trouv√©es")
                                if unmatched:
                                    st.warning(f"‚ö†Ô∏è {len(unmatched)} URLs non match√©es")
                                
                                # Filtre par confiance
                                filtered_matches = [
                                    match for match in matches 
                                    if match.get('confidence', 0) >= confidence_threshold
                                ]
                                
                                if len(filtered_matches) < len(matches):
                                    excluded = len(matches) - len(filtered_matches)
                                    st.info(f"üéØ {excluded} matches exclus (confiance < {confidence_threshold})")
                                
                                all_matches.extend(filtered_matches)
                                all_unmatched.extend(unmatched)
                        
                        # Sauvegarde des r√©sultats dans le cache
                        gpt_results = {
                            "all_matches": all_matches,
                            "all_unmatched": all_unmatched,
                            "missing_langs": missing_langs,
                            "old_grouped": old_grouped
                        }
                        
                        cache_file = cache_manager.save_gpt_cache(
                            old_urls, new_urls, contexte_metier, temperature, gpt_results
                        )
                        st.success(f"üíæ R√©sultats sauvegard√©s en cache: {cache_file}")
                    
                    # Les variables all_matches, all_unmatched, etc. sont maintenant disponibles
                    # que ce soit depuis le cache ou depuis l'API GPT
                    
                    # Sprint 3 - Traitement intelligent des URLs non match√©es
                    fallback_302_results = {}
                    fallback_302_csv_data = []
                    
                    if enable_fallback_302 and all_unmatched:
                        st.subheader("üîÑ Fallback intelligent 302 (Sprint 3)")
                        
                        # Initialisation du gestionnaire fallback 302
                        intelligent_fallback = Fallback302Intelligent(
                            enable_fallback_302=True,
                            default_fallback_lang=fallback_lang
                        )
                        
                        # Traitement des URLs non match√©es
                        ai_session_results = {
                            "matched_urls": all_matches,
                            "unmatched_urls": all_unmatched
                        }
                        
                        fallback_302_results = intelligent_fallback.process_ai_results(ai_session_results)
                        
                        # Affichage des r√©sultats fallback
                        nb_fallback = len(fallback_302_results["fallback_302_redirects"])
                        if nb_fallback > 0:
                            st.success(f"‚úÖ {nb_fallback} redirections 302 fallback g√©n√©r√©es")
                            
                            # Pr√©paration des donn√©es CSV pour export
                            fallback_302_csv_data = fallback_302_results["csv_export_data"]
                            
                            # Affichage d'un √©chantillon
                            with st.expander(f"üîç Aper√ßu des {min(3, nb_fallback)} premi√®res redirections 302"):
                                for redirect in fallback_302_results["fallback_302_redirects"][:3]:
                                    st.code(f"Redirect 302 {redirect['old_url']} {redirect['target_url']}")
                        else:
                            st.info("‚ÑπÔ∏è Aucune URL non match√©e n√©cessitant de fallback 302")
                    elif not enable_fallback_302 and all_unmatched:
                        st.warning(f"‚ö†Ô∏è {len(all_unmatched)} URLs non match√©es (fallback 302 d√©sactiv√©)")
                    
                    # Fin Sprint 3 - Fallback intelligent
                    
                    # G√©n√©ration des redirections 302 pour langues manquantes
                    redirects_302 = []
                    if missing_langs:
                        redirects_302 = fallback_manager.generate_302_redirects(
                            missing_langs, old_grouped, fallback_lang
                        )
                    
                    # G√©n√©ration du fichier .htaccess final avec URLs absolues
                    st.subheader("üìÑ Fichier .htaccess avec URLs absolues")
                    
                    # Pr√©paration de toutes les redirections avec URLs absolues
                    all_redirects = []
                    
                    # 1. Ajout des matches IA avec commentaires (301)
                    for match in all_matches:
                        source = match['ancienne']
                        target = match['nouvelle']
                        
                        # Conversion target vers domaine cible (URLs absolues)
                        target_absolute = convert_url_domain(target, target_domain)
                        confidence = match.get('confidence', 0)
                        reason = match.get('raison', '')
                        
                        all_redirects.append({
                            'type': '301',
                            'source': source,
                            'target': target_absolute,
                            'comment': f"# {reason} (confiance: {confidence:.2f})",
                            'confidence': confidence
                        })
                    
                    # 2. Ajout des fallback 302 intelligents avec URLs absolues
                    if enable_fallback_302 and all_unmatched:
                        for unmatched_url in all_unmatched:
                            language = extract_language_from_url_path(unmatched_url)
                            target_fallback = f"{target_domain}/{language}/"
                            
                            all_redirects.append({
                                'type': '302',
                                'source': unmatched_url,
                                'target': target_fallback,
                                'comment': f"# Fallback 302 vers la home {language.upper()}",
                                'confidence': 0
                            })
                    
                    # 3. Ajout des redirections 302 pour langues manquantes  
                    if missing_langs:
                        redirects_302 = fallback_manager.generate_302_redirects(
                            missing_langs, old_grouped, fallback_lang
                        )
                        for source, target_relative in redirects_302:
                            target_absolute = f"{target_domain}{target_relative}"
                            language = extract_language_from_url_path(source)
                            
                            all_redirects.append({
                                'type': '302',
                                'source': source,
                                'target': target_absolute,
                                'comment': f"# Langue {language.upper()} non migr√©e",
                                'confidence': 0
                            })
                    
                    # 4. Filtrage des doublons et source == cible
                    filtered_redirects = remove_duplicate_redirects(all_redirects)
                    
                    # 5. Construction du .htaccess final
                    htaccess_lines = [
                        "# Redirections 301/302 avec URLs absolues",
                        "# Fire Snake 301 - Sprint 3 Enhanced",
                        "# Format: Source absolue ‚Üí Target absolue",
                        ""
                    ]
                    
                    # S√©paration 301 et 302 pour meilleure organisation
                    redirects_301 = [r for r in filtered_redirects if r['type'] == '301']
                    redirects_302 = [r for r in filtered_redirects if r['type'] == '302']
                    
                    # Ajout des 301 avec commentaires IA
                    if redirects_301:
                        htaccess_lines.extend([
                            "# ===================================================",
                            "# REDIRECTIONS 301 - MATCHING IA S√âMANTIQUE",
                            "# ===================================================",
                            ""
                        ])
                        
                        for redirect in redirects_301:
                            htaccess_lines.append(redirect['comment'])
                            htaccess_lines.append(f"Redirect 301 {redirect['source']} {redirect['target']}")
                            htaccess_lines.append("")
                    
                    # Ajout des 302 avec commentaires simplifi√©s
                    if redirects_302:
                        htaccess_lines.extend([
                            "# ===================================================",
                            "# REDIRECTIONS 302 - FALLBACK INTELLIGENT",
                            "# URLs orphelines ‚Üí Page d'accueil langue",
                            "# ===================================================",
                            ""
                        ])
                        
                        for redirect in redirects_302:
                            htaccess_lines.append(redirect['comment'])
                            htaccess_lines.append(f"Redirect 302 {redirect['source']} {redirect['target']}")
                            htaccess_lines.append("")
                    
                    # 6. G√©n√©ration du contenu final .htaccess
                    htaccess_content = "\n".join(htaccess_lines)
                    
                    # 7. R√©sum√© statistiques en console (BONUS)
                    st.subheader("üìä R√©sum√© de g√©n√©ration")
                    
                    total_301 = len(redirects_301)
                    total_302 = len(redirects_302)
                    total_redirects = total_301 + total_302
                    total_old_urls = len(old_urls)
                    
                    # Calcul du taux de correspondance IA
                    if total_old_urls > 0:
                        taux_correspondance = (total_301 / total_old_urls) * 100
                    else:
                        taux_correspondance = 0
                    
                    # Affichage des m√©triques
                    col_stats1, col_stats2, col_stats3, col_stats4 = st.columns(4)
                    
                    with col_stats1:
                        st.metric("‚úÖ Redirections 301", total_301, help="Correspondances IA trouv√©es")
                    
                    with col_stats2:
                        st.metric("‚Ü™Ô∏è Redirections 302", total_302, help="Fallback automatiques")
                    
                    with col_stats3:
                        st.metric("üéØ Taux correspondance", f"{taux_correspondance:.1f}%", help="% URLs match√©es par IA")
                    
                    with col_stats4:
                        st.metric("üîß Total redirections", total_redirects, help="301 + 302 combin√©es")
                    
                    # Message r√©capitulatif style console
                    st.code(f"""‚úÖ {total_301} redirections 301 g√©n√©r√©es
‚Ü™Ô∏è {total_302} redirections 302 fallback  
üéØ Taux de correspondance IA : {taux_correspondance:.2f}%
üìÑ Fichier .htaccess pr√™t pour production avec URLs absolues""", language="text")
                    
                    # Sauvegarde automatique du fichier .htaccess
                    htaccess_file_path = cache_manager.save_htaccess_file(
                        htaccess_content, old_urls, new_urls, "ai_semantic"
                    )
                    st.success(f"üíæ Fichier .htaccess sauvegard√©: {htaccess_file_path}")
                    
                    # Affichage et t√©l√©chargement
                    st.code(htaccess_content, language="apache")
                    
                    # Sprint 3.5 - Ajout de la colonne pour export XLS Balt
                    if OPENPYXL_AVAILABLE:
                        if enable_fallback_302 and fallback_302_csv_data:
                            col_dl1, col_dl2, col_dl3, col_dl4, col_dl5 = st.columns(5)
                        else:
                            col_dl1, col_dl2, col_dl3, col_dl4 = st.columns(4)
                    else:
                        # Fallback si openpyxl non disponible
                        if enable_fallback_302 and fallback_302_csv_data:
                            col_dl1, col_dl2, col_dl3, col_dl4 = st.columns(4)
                        else:
                            col_dl1, col_dl2, col_dl3 = st.columns(3)
                    
                    with col_dl1:
                        st.download_button(
                            label="‚¨áÔ∏è T√©l√©charger .htaccess",
                            data=htaccess_content,
                            file_name="htaccess_ai_semantic.txt",
                            mime="text/plain"
                        )
                    
                    with col_dl2:
                        # G√©n√©ration du CSV d√©taill√© avec URLs absolues
                        csv_data = [["Source", "Target", "Type", "Confidence", "Reason", "Language"]]
                        
                        # Utilisation des redirections filtr√©es pour le CSV
                        for redirect in filtered_redirects:
                            csv_data.append([
                                redirect['source'],
                                redirect['target'],  # URLs absolues maintenant
                                redirect['type'],
                                f"{redirect.get('confidence', 0):.2f}",
                                redirect['comment'].replace('# ', ''),  # Nettoyage du commentaire
                                extract_language_from_url_path(redirect['source'])
                            ])
                        
                        csv_buffer = io.StringIO()
                        csv_writer = csv.writer(csv_buffer)
                        csv_writer.writerows(csv_data)
                        
                        st.download_button(
                            label="‚¨áÔ∏è CSV Complet",
                            data=csv_buffer.getvalue(),
                            file_name="redirections_absolues_301_302.csv",
                            mime="text/csv",
                            help="CSV avec toutes les redirections (URLs absolues)"
                        )
                    
                    with col_dl3:
                        # Statistiques d√©taill√©es
                        stats = {
                            "total_301": len(all_matches),
                            "total_302": len(redirects_302),
                            "unmatched": len(all_unmatched),
                            "languages_processed": len(common_langs),
                            "missing_languages": len(missing_langs)
                        }
                        
                        st.metric("üìä Stats", f"301: {stats['total_301']} | 302: {stats['total_302']}")
                        
                        if all_unmatched:
                            with st.expander("‚ö†Ô∏è URLs non match√©es"):
                                for url in all_unmatched:
                                    st.text(url)
                    
                    # Sprint 3 - Bouton export CSV fallback 302
                    if enable_fallback_302 and fallback_302_csv_data:
                        with col_dl4:
                            # Export CSV des fallback 302
                            intelligent_fallback = Fallback302Intelligent()
                            fallback_csv_content = intelligent_fallback.export_to_csv(fallback_302_csv_data)
                            
                            st.download_button(
                                label="üîÑ CSV Fallback 302",
                                data=fallback_csv_content,
                                file_name="fallback_302_urls.csv",
                                mime="text/csv",
                                help="URLs non match√©es avec redirections 302 temporaires"
                            )
                    
                    # Sprint 3.5 - Export XLS Balt (nouveau bouton)
                    if OPENPYXL_AVAILABLE:
                        if enable_fallback_302 and fallback_302_csv_data:
                            col_xls = col_dl5  # 5√®me colonne si fallback 302 activ√©
                        else:
                            col_xls = col_dl4  # 4√®me colonne sinon
                            
                        with col_xls:
                            # G√©n√©ration du fichier XLS Balt en m√©moire
                            try:
                                import tempfile
                                with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
                                    balt_file_path = generate_balt_xlsx(filtered_redirects, tmp_file.name)
                                    
                                    # Lecture du fichier pour t√©l√©chargement
                                    with open(balt_file_path, 'rb') as f:
                                        balt_data = f.read()
                                    
                                    # Nettoyage du fichier temporaire
                                    try:
                                        os.unlink(balt_file_path)
                                    except:
                                        pass
                                    
                                    st.download_button(
                                        label="üíæ Export XLS (Balt)",
                                        data=balt_data,
                                        file_name="redirections_balt.xlsx",
                                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                        help="Format XLS compatible Septeo BALT CMS (2 colonnes Source/Target uniquement)"
                                    )
                            except Exception as e:
                                st.error(f"‚ùå Erreur export XLS: {str(e)}")
                                st.info("üí° V√©rifiez que openpyxl est install√©: `pip install openpyxl`")
                
                except AIMatchingError as e:
                    st.error(f"‚ùå Erreur IA: {str(e)}")
                except Exception as e:
                    st.error(f"‚ùå Erreur inattendue: {str(e)}")


def interface_classique():
    """Interface classique sans IA (pour compatibilit√©)"""
    
    st.header("üîÑ Mode classique (sans IA)")
    st.info("‚ÑπÔ∏è Mode traditionnel avec mapping alphab√©tique")
    
    # Interface simplifi√©e avec sitemap pour le nouveau site
    st.header("üîÑ Collecte des URLs")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üîó Ancien site (en ligne)")
        
        # Option 1: Scraping automatique
        st.markdown("**Option 1: Scraping automatique**")
        old_url = st.text_input("URL de l'ancien site √† scraper", placeholder="https://ancien-site.com")
        max_pages = st.number_input("Nombre max de pages", 50, 500, 200)
        
        if st.button("üï∑Ô∏è Lancer le scraping"):
            if old_url:
                with st.spinner("Scraping en cours..."):
                    old_urls = crawl_site_with_fallback(old_url, max_pages=max_pages)
                    st.session_state.old_urls_scraped = old_urls
                    st.success(f"‚úÖ {len(old_urls)} URLs scrap√©es avec succ√®s!")
            else:
                st.error("‚ùå Veuillez entrer l'URL de l'ancien site.")
        
        # Option 2: Upload CSV/TXT
        st.markdown("**Option 2: Upload fichier**")
        old_file = st.file_uploader("Fichier URLs anciennes", type=['csv', 'txt'], key="old_file")
        
        # Option 3: Saisie manuelle
        st.markdown("**Option 3: Saisie manuelle**") 
        old_manual = st.text_area("URLs anciennes (une par ligne)", height=150, key="old_manual_classic")
        
    with col2:
        st.subheader("üéØ Nouveau site (pr√©production)")
        
        # Option 1: Parsing sitemap XML
        st.markdown("**Option 1: Sitemap XML (recommand√©)**")
        sitemap_url = st.text_input("URL du sitemap", placeholder="https://nouveau-site.com/sitemap.xml")
        
        if st.button("üó∫Ô∏è Parser le sitemap"):
            if sitemap_url:
                with st.spinner("Parsing du sitemap..."):
                    new_urls = parse_sitemap(sitemap_url)
                    st.session_state.new_urls_sitemap = new_urls
                    st.success(f"‚úÖ {len(new_urls)} URLs extraites du sitemap!")
            else:
                st.error("‚ùå Veuillez entrer l'URL du sitemap.")
        
        # Option 2: Upload CSV/TXT
        st.markdown("**Option 2: Upload fichier**")
        new_file = st.file_uploader("Fichier URLs nouvelles", type=['csv', 'txt'], key="new_file")
        
        # Option 3: Saisie manuelle
        st.markdown("**Option 3: Saisie manuelle**")
        new_manual = st.text_area("URLs nouvelles (une par ligne)", height=150, key="new_manual_classic")
    
    # Collecte finale des URLs
    old_urls = []
    new_urls = []
    
    # Anciennes URLs
    if hasattr(st.session_state, 'old_urls_scraped'):
        old_urls = st.session_state.old_urls_scraped
    elif old_file:
        content = old_file.read().decode()
        old_urls = [url.strip() for url in content.split('\n') if url.strip()]
    elif old_manual:
        old_urls = [url.strip() for url in old_manual.split('\n') if url.strip()]
    
    # Nouvelles URLs
    if hasattr(st.session_state, 'new_urls_sitemap'):
        new_urls = st.session_state.new_urls_sitemap
    elif new_file:
        content = new_file.read().decode()
        new_urls = [url.strip() for url in content.split('\n') if url.strip()]
    elif new_manual:
        new_urls = [url.strip() for url in new_manual.split('\n') if url.strip()]
    
    # Affichage des URLs collect√©es
    if old_urls and new_urls:
        display_url_preview(old_urls, new_urls)
        
        # G√©n√©ration des redirections classiques
        if st.button("‚ö° G√©n√©rer les redirections", type="primary"):
            with st.spinner("G√©n√©ration en cours..."):
                htaccess_content, csv_data, error = generate_redirections_from_lists(old_urls, new_urls)
                
                if error:
                    st.error(f"‚ùå Erreur: {error}")
                else:
                    st.success("‚úÖ Redirections g√©n√©r√©es avec succ√®s!")
                    
                    # Affichage du contenu .htaccess
                    st.subheader("üìÑ Contenu .htaccess")
                    st.code(htaccess_content, language="apache")
                    
                    # Boutons de t√©l√©chargement
                    col_dl1, col_dl2 = st.columns(2)
                    with col_dl1:
                        st.download_button(
                            label="‚¨áÔ∏è T√©l√©charger .htaccess",
                            data=htaccess_content,
                            file_name="htaccess_redirections.txt",
                            mime="text/plain"
                        )
                    with col_dl2:
                        csv_buffer = io.StringIO()
                        csv_writer = csv.writer(csv_buffer)
                        csv_writer.writerows(csv_data)
                        st.download_button(
                            label="‚¨áÔ∏è T√©l√©charger CSV",
                            data=csv_buffer.getvalue(),
                            file_name="redirections_audit.csv",
                            mime="text/csv"
                        )
    
    elif old_urls or new_urls:
        st.warning("‚ö†Ô∏è Veuillez fournir les URLs pour l'ancien ET le nouveau site.")


def display_url_preview(old_urls, new_urls):
    """Affiche un aper√ßu des URLs d√©tect√©es"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(f"üîó Anciennes URLs d√©tect√©es ({len(old_urls)})")
        if old_urls:
            # Affiche les 10 premi√®res URLs
            preview_old = old_urls[:10]
            for url in preview_old:
                st.text(url)
            if len(old_urls) > 10:
                st.text(f"... et {len(old_urls) - 10} autres")
        else:
            st.warning("Aucune URL d√©tect√©e")
    
    with col2:
        st.subheader(f"üéØ Nouvelles URLs d√©tect√©es ({len(new_urls)})")
        if new_urls:
            # Affiche les 10 premi√®res URLs
            preview_new = new_urls[:10]
            for url in preview_new:
                st.text(url)
            if len(new_urls) > 10:
                st.text(f"... et {len(new_urls) - 10} autres")
        else:
            st.warning("Aucune URL d√©tect√©e")


def generate_redirections_from_lists(old_urls, new_urls):
    """G√©n√®re les redirections √† partir de deux listes d'URLs"""
    try:
        generator = RedirectGenerator()
        
        # Convertit les listes en texte pour le g√©n√©rateur existant
        old_text = '\n'.join(old_urls)
        new_text = '\n'.join(new_urls)
        
        htaccess_content, csv_data = generator.generate_redirections(old_text, new_text)
        return htaccess_content, csv_data, None
        
    except Exception as e:
        return None, None, str(e)