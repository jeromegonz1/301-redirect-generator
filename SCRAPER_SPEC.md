# ðŸ“„ SCRAPER_SPEC.md â€“ SpÃ©cification du module de scraping

## ðŸ§­ Objectif

Ce module est conÃ§u pour extraire automatiquement la liste des URLs internes d'un site web (ancien ou nouveau) Ã  partir d'une **URL racine**.  
Il permet d'alimenter l'outil 301 Redirect Generator **sans intervention manuelle**.  
Il remplace le copier-coller d'un sitemap ou d'un export GA / SEO par un crawl intelligent.

---

## ðŸ§  Fonction attendue

- L'utilisateur saisit **l'URL d'un ancien site** et **l'URL d'un site nouveau / prÃ©prod**
- Le module effectue un **crawl automatique** jusqu'Ã  un certain nombre de pages (par dÃ©faut : 200 max)
- Il retourne :
  - Une **liste d'URLs complÃ¨tes** pour le site A (ancien)
  - Une **liste d'URLs relatives** pour le site B (nouveau / prÃ©prod)

---

## ðŸ§± Stack recommandÃ©e

| Ã‰lÃ©ment | Choix |
|--------|-------|
| Langage | Python 3.x |
| HTTP client | `requests` |
| Parsing HTML | `BeautifulSoup` |
| Crawl queue | `deque` (`collections`) |
| Normalisation URL | `urllib.parse`, `tldextract` |
| Respect des contraintes SEO | Optionnel / dÃ©sactivable (robots.txt, nofollow) |
| Alternatives futures | Interface extensible vers `Playwright`, `Scrapy` |

---

## ðŸ” AccÃ¨s prÃ©prod

PrÃ©voir une capacitÃ© d'accÃ¨s **aux environnements restreints** :

| Cas | Solution |
|-----|----------|
| `robots.txt` bloquant | Overridable avec `User-Agent: 301-Redirect-Bot` |
| Meta `noindex/nofollow` | IgnorÃ©s par dÃ©faut |
| Authentification | Support `Basic Auth` ou `Bearer Token` (Ã  injecter dans headers) |
| PrÃ©prod privÃ©e (non exposÃ©e) | Fallback : copier-coller / upload manuel |

---

## ðŸ“¥ EntrÃ©e attendue

```python
crawl_site(
    url_root: str,
    max_pages: int = 200,
    user_agent: str = "301-Redirect-Bot",
    auth: Optional[Tuple[str, str]] = None,
    headers: Optional[Dict[str, str]] = None
) -> List[str]
```

## ðŸ“¤ Sortie

Une liste d'URLs propres, normalisÃ©es, sans doublon :

```python
[
    "https://vieux-site.com/contact",
    "https://vieux-site.com/hebergement/chalet",
    ...
]
```

Ou pour le nouveau site :

```python
[
    "/contact",
    "/hebergements/chalets",
    ...
]
```

## ðŸ§ª RÃ¨gles fonctionnelles

Le crawler :

- suit les liens internes (`<a href>`)
- ignore les liens externes (autres domaines)
- nettoie les URLs (sans query string, trailing slash)
- Ã©vite les doublons
- ignore les ancres (#)
- Le crawl est large mais limitÃ© : max 200 pages par domaine
- Les URLs peuvent Ãªtre triÃ©es ou conservÃ©es dans l'ordre d'apparition (au choix)

## ðŸ§© Cas Ã  gÃ©rer

| Cas | Comportement |
|-----|-------------|
| HTTP 403 / 404 | IgnorÃ© avec log silencieux |
| Timeout / exception | IgnorÃ© avec log silencieux |
| Lien vide ou ancre seule (#) | IgnorÃ© |
| Boucle infinie | EmpÃªchÃ©e via set() de pages visitÃ©es |

## ðŸ”§ Exemple d'implÃ©mentation de base

```python
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import requests
from collections import deque

def crawl_site(url_root: str, max_pages=200) -> list:
    visited = set()
    to_visit = deque([url_root])
    collected = []

    while to_visit and len(collected) < max_pages:
        url = to_visit.popleft()
        if url in visited:
            continue
        visited.add(url)
        try:
            res = requests.get(url, timeout=5, headers={'User-Agent': '301-Redirect-Bot'})
            soup = BeautifulSoup(res.text, "html.parser")
            for link in soup.find_all("a", href=True):
                href = urljoin(url, link['href'])
                if href.startswith(url_root) and href not in visited:
                    to_visit.append(href)
            collected.append(url)
        except:
            continue
    return collected
```

## ðŸ”„ Fallback si Ã©chec du crawl

Si le site ne rÃ©pond pas / bloquÃ© / inaccessible :

- Retourne None ou liste vide
- Affiche un message UX : "Le site est inaccessible, veuillez coller les URLs ou importer un fichier sitemap"

## ðŸ§ª Tests attendus (Pytest)

- `test_crawl_site_respects_max_pages()`
- `test_crawl_removes_query_strings_and_fragments()`
- `test_crawl_ignores_external_domains()`
- `test_crawl_with_basic_auth_header()`

## ðŸ”„ Fichiers concernÃ©s

- `src/scraper.py` â†’ contient `crawl_site()` et helpers
- `tests/test_scraper.py` â†’ tests unitaires du module
- `main.py` â†’ appelle `crawl_site()` si mode scraping sÃ©lectionnÃ©

## ðŸ§™ Bonus futur

- Mode "deep crawl" avec rÃ¨gles personnalisÃ©es
- Scraping asynchrone (via aiohttp)
- UI avec visualisation des liens
- IntÃ©gration dans Fire Salamander ðŸ‘€